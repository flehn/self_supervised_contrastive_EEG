{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b018bf-9fe9-4512-a1b2-06b8bc5a5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different EEG Augmentation functions\n",
    "#https://github.com/braindecode/braindecode.git\n",
    "\n",
    "#TorchEEG manuell import:\n",
    "#https://github.com/torcheeg/torcheeg.git\n",
    "\n",
    "#SimCLR TorchEEG:\n",
    "#https://torcheeg.readthedocs.io/en/latest/generated/torcheeg.trainers.SimCLRTrainer.html#torcheeg.trainers.SimCLRTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff31f0-feb4-4739-9b81-e129d149d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97408b72-5887-4089-8ffe-23839a266564",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task to DO:\n",
    "\n",
    "Train on Deap, SeedIV\n",
    "Downstream on Seed \n",
    "\n",
    "Use different batch_size: 512 vs 1024\n",
    "Use different Augmentation:\n",
    "Random Noise vs EEGNoise\n",
    "\n",
    "Raw vs DifferentialEntropyFeature. \n",
    "\n",
    "Predict on Trials, vs predict on leave one Subj out\n",
    "\n",
    "'''\n",
    "\n",
    "#Experiment1:\n",
    "'''\n",
    "Train on Deap, SeedIV\n",
    "Use Differntial Entropy\n",
    "Batch_size: 512\n",
    "X1: Random_Noise std 0.1\n",
    "X2: Random_Noise std 0.4\n",
    "Question: Does it create meaningful representations for Downstream on SEED?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15d1c0e5-0d7b-4704-9b04-f944f4caf66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_paths\n",
    "DATA_PATH = './' #where data is saved\n",
    "TMP_DATA = './Tmp_Datasets'\n",
    "CHECKPOINT_PATH = './saved_models' # Path to the folder where the pretrained models are saved\n",
    "\n",
    "# Path to the folder where the training dataset is located\n",
    "SEED_PATH = './SEED/SEED_EEG/ExtractedFeatures/Subj'\n",
    "SEEDiv_PATH = './SEED/SEED_IV/ExtractedFeatures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20bf9ed-55a9-49bf-b107-9e11db3ee899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_WORKERS: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 17:15:12,628 DEBUG MainThread git.cmd Popen(['git', 'version'], cwd=/workspace, stdin=None, shell=False, universal_newlines=False)\n",
      "2023-12-04 17:15:12,630 DEBUG MainThread git.cmd Popen(['git', 'version'], cwd=/workspace, stdin=None, shell=False, universal_newlines=False)\n",
      "2023-12-04 17:15:12,642 DEBUG MainThread wandb.docker.auth Trying paths: ['/root/.docker/config.json', '/root/.dockercfg']\n",
      "2023-12-04 17:15:12,643 DEBUG MainThread wandb.docker.auth No config file found\n",
      "2023-12-04 17:15:12,692 DEBUG MainThread sentry_sdk.errors [Tracing] Create new propagation context: {'trace_id': 'fc526cfc106b4dec8938824b2e50a37c', 'span_id': 'bf2f435d75905b1a', 'parent_span_id': None, 'dynamic_sampling_context': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  4 17:15:13 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A40                     On  | 00000000:52:00.0 Off |                    0 |\n",
      "|  0%   36C    P0              75W / 300W |    348MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Your runtime has 540.7 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n",
      "Is the GPU available? True\n",
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "gpus = [0]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, gpus))\n",
    "print(f'NUM_WORKERS: {NUM_WORKERS}')\n",
    "\n",
    "import numpy as np\n",
    "import io\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "#Import Pytorch\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.nn.functional import pad\n",
    "    from torch import Tensor\n",
    "    from torch.fft import fft, ifft\n",
    "    import torch.optim as optim\n",
    "    import sklearn\n",
    "    import seaborn \n",
    "\n",
    "except ModuleNotFoundError: \n",
    "    !pip install seaborn \n",
    "    !pip install torch\n",
    "    !pip install scikit-learn\n",
    "    import seaborn \n",
    "    import sklearn\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.nn.functional import pad\n",
    "    from torch import Tensor\n",
    "    import torch.optim as optim\n",
    "    from torch.fft import fft, ifft\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import lightning.pytorch as pl\n",
    "    #import pytorch_lightning as pl\n",
    "    import torchmetrics\n",
    "    import torcheeg\n",
    "    import wandb\n",
    "    \n",
    "    \n",
    "except ModuleNotFoundError: \n",
    "    !pip install wandb\n",
    "    !pip install comet-ml\n",
    "    !pip install lightning\n",
    "    #!pip install --quiet pytorch-lightning>=1.4\n",
    "    !pip install torcheeg\n",
    "    !pip install torchmetrics\n",
    "    import wandb\n",
    "    #import comet_ml\n",
    "    import lightning.pytorch as pl\n",
    "    #import pytorch_lightning as pl\n",
    "    import torcheeg\n",
    "    import torchmetrics\n",
    "\n",
    "\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.loggers import CometLogger\n",
    "from lightning.pytorch.callbacks import GradientAccumulationScheduler, LearningRateMonitor, ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "#from pytorch_lightning.loggers import WandbLogger\n",
    "#from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "\n",
    "from typing import Any, Tuple, List\n",
    "from torcheeg.datasets import SEEDDataset, DEAPDataset\n",
    "from torcheeg import transforms, model_selection\n",
    "\n",
    "\n",
    "#Create custom dataset:(inherits from torch.utils.data.Dataset)\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "import scipy\n",
    "import scipy.io as scio #for loading raw EEG\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "\n",
    "import PIL\n",
    "\n",
    "from matplotlib import colors\n",
    "from mne.viz import circular_layout\n",
    "from mne_connectivity.viz import plot_connectivity_circle\n",
    "from pylab import cm\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "#Local imports \n",
    "from Plotting import PlotEEG\n",
    "from Datasets import create_dataset\n",
    "from Augmentations import ContrastiveTransformations\n",
    "from AllModels import ArjunViT, Trainer_class2\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')\n",
    "\n",
    "gpu_avail = torch.cuda.is_available()\n",
    "print(f\"Is the GPU available? {gpu_avail}\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439d840-b7e9-40ac-b3e1-b7e715d64964",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameter\n",
    "'''\n",
    "\n",
    "#EEG - Data:\n",
    "chunk_size=200 #length of a sample\n",
    "channels = 62\n",
    "overlap=0 \n",
    "\n",
    "#NonSSL Models:\n",
    "nr_devices = 'auto'\n",
    "batch_size=64\n",
    "nr_classes=3 #Number of target classes\n",
    "nr_epochs=30\n",
    "lr=4e-4  # Conformer paper uses 0.0002\n",
    "weight_decay=1e-3\n",
    "\n",
    "#SSL-only Model\n",
    "batch_sizeSSL = 1024 #based on https://arxiv.org/pdf/2002.05709.pdf\n",
    "lrSSL= 5e-4 #0.0001 \n",
    "weight_decaySSL = 1e-6 #based on https://arxiv.org/pdf/2002.05709.pdf\n",
    "temperatureSSL = 0.05 #based on mohsenvand2020a\n",
    "\n",
    "#https://arxiv.org/pdf/2007.16104.pdf\n",
    "#The Adam optimizer [50] with β1 = 0.9 and β2 = 0.999 and learning rate 5 × 10−4 was used.\n",
    "\n",
    "#https://arxiv.org/pdf/2109.09559.pdf\n",
    "#We used an Adam optimizer [85] with a cosine annealing learning rate scheduler and a three-time warm restart [86]. \n",
    "#The initial learning rate was set to 0.0007, and the weight decay was set to 0.015 empirically. \n",
    "'''\n",
    "In the contrastive learning procedure, we used stratified\n",
    "normalization [84] during training. In stratified normalization, we concatenated the same channel of different samples from one subject in the minibatch together and conducted z-score normalization. The stratified normalization\n",
    "was applied to inputs of the base encoder, outputs of average pooling, and outputs of the temporal convolution in\n",
    "the projector. F\n",
    "'''\n",
    "\n",
    "if str(device).startswith(\"cuda\"):\n",
    "    torch.set_float32_matmul_precision('high') #Increase performance\n",
    "    \n",
    "#Logger\n",
    "project_name='Experiment_11'\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "#198d0b46f74d1861672c12c46489054b043161f2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31746d9-26c1-482c-97e5-14f19f08c5a2",
   "metadata": {},
   "source": [
    "## Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "943c037f-c442-40d0-9536-9a120dab3d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED_PATH_Subj1\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj1, reading from path...\n",
      "SEED_PATH_Subj2\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj2, reading from path...\n",
      "SEED_PATH_Subj3\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj3, reading from path...\n",
      "SEED_PATH_Subj4\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj4, reading from path...\n",
      "SEED_PATH_Subj5\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj5, reading from path...\n",
      "SEED_PATH_Subj6\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj6, reading from path...\n",
      "SEED_PATH_Subj7\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj7, reading from path...\n",
      "SEED_PATH_Subj8\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj8, reading from path...\n",
      "SEED_PATH_Subj9\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj9, reading from path...\n",
      "SEED_PATH_Subj10\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj10, reading from path...\n",
      "SEED_PATH_Subj11\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj11, reading from path...\n",
      "SEED_PATH_Subj12\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj12, reading from path...\n",
      "SEED_PATH_Subj13\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj13, reading from path...\n",
      "SEED_PATH_Subj14\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj14, reading from path...\n",
      "SEED_PATH_Subj15\n",
      "dataset already exists at path ./Tmp_Datasets/SEED_PATH_Subj15, reading from path...\n",
      "[SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj1',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj1',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f817260b250>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj2',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj2',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f838d1893f0>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj3',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj3',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f8178c92950>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj4',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj4',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f817260ae60>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj5',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj5',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f817260b640>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj6',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj6',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f817260b760>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj7',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj7',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f817260b880>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj8',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj8',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f817260b9a0>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj9',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj9',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f838d1895a0>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj10',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj10',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f817260bb50>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj11',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj11',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f817260b520>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj12',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj12',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f817260b6d0>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj13',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj13',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f817260b910>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj14',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj14',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f817260bc70>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182, SEEDFeatureDataset(\n",
      "    io_path='./Tmp_Datasets/SEED_PATH_Subj15',\n",
      "    io_size=10485760,\n",
      "    io_mode='lmdb',\n",
      "    in_memory=False,\n",
      "    root_path='./SEED/SEED_EEG/ExtractedFeatures/Subj15',\n",
      "    feature=['de_movingAve'],\n",
      "    num_channel=62,\n",
      "    online_transform=None,\n",
      "    offline_transform=Compose(\n",
      "    BaselineRemoval(apply_to_baseline=False),\n",
      "    ToTensor(apply_to_baseline=False),\n",
      "    To2d(apply_to_baseline=False),\n",
      "    MeanStdNormalize(apply_to_baseline=False, mean=None, std=None, axis=None)\n",
      "),\n",
      "    label_transform=Compose(\n",
      "    Select(key='emotion'),\n",
      "    Lambda(lambd=<function <lambda> at 0x7f817260bd90>, targets=[Ellipsis])\n",
      "),\n",
      "    before_trial=None,\n",
      "    after_trial=None,\n",
      "    num_worker=96,\n",
      "    verbose=True\n",
      ")\n",
      "length=10182]\n"
     ]
    }
   ],
   "source": [
    "#Leave one out\n",
    "path = './SEED/SEED_EEG/ExtractedFeatures/Subj'\n",
    "from torcheeg.datasets import SEEDFeatureDataset\n",
    "SEED_PATH_Subj = 'SEED_PATH_Subj'\n",
    "Datasets = []\n",
    "for i in range(1,16):\n",
    "    name = SEED_PATH_Subj + str(i)\n",
    "    file_path = path + str(i)\n",
    "    print(name)\n",
    "    \n",
    "    SEED_dataset_path = os.path.join(TMP_DATA, name)\n",
    "    Datasets.append(SEEDFeatureDataset(\n",
    "                      io_path=SEED_dataset_path, #location of loaded and transformed dataset\n",
    "                      root_path=file_path, #location of original dataset \n",
    "                      feature=['de_movingAve'],\n",
    "                      offline_transform=transforms.Compose([\n",
    "                                              transforms.BaselineRemoval(),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.To2d(),\n",
    "                                              transforms.MeanStdNormalize(),\n",
    "                                              ]),\n",
    "                                          label_transform=transforms.Compose([\n",
    "                                              transforms.Select('emotion'),\n",
    "                                              transforms.Lambda(lambda x: x + 1)]),\n",
    "                                           num_worker=NUM_WORKERS\n",
    "                                          ))\n",
    "print(Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698acef-b449-46b3-83da-9dfcb187a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b829e9c-20bb-465f-b11c-0b8427f2d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_info = !df -h\n",
    "disk_info = '\\n'.join(disk_info)\n",
    "print(disk_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fba9094-d24e-4721-b494-27920bffafe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised(model, batch_size, train_data, test_data, model_name, num_classes, logger=None,  max_epochs=100, **kwargs):\n",
    "    \n",
    "    \n",
    "    #Save Models in CHECKPOINT_PATH with model_name \n",
    "    save_model_callback = ModelCheckpoint(dirpath=CHECKPOINT_PATH,filename=model_name)\n",
    "    #Accumlate the gradients: in this case from 0-4 epoch it will accumlate 8 batches, from 5-8 it will accumlate 4 and finally 1\n",
    "    accumulator_callback = GradientAccumulationScheduler(scheduling={0: 8, 4: 4, 8: 1})\n",
    "    #EarlyStopping: stop and skip the rest of the current epoch when val_loss cant be reduced anymore\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\")\n",
    "    \n",
    "    print(f'model: {model_name}, \\n default_root: {CHECKPOINT_PATH}')\n",
    "    trainer = pl.Trainer(\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices='auto',\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[save_model_callback,\n",
    "                                    LearningRateMonitor(\"epoch\"),\n",
    "                                    accumulator_callback,\n",
    "                                    early_stop_callback,\n",
    "                                   ],\n",
    "                         #enable_progress_bar=False,\n",
    "                         check_val_every_n_epoch=5,\n",
    "                         logger=logger,\n",
    "                        )\n",
    "    \n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True,\n",
    "                                   drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False,\n",
    "                                  drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    print(f'Batch_size: {batch_size}')\n",
    "    print(f'train_loader: {len(train_loader)}')\n",
    "\n",
    "    model = Trainer_class2(model, lr, weight_decay, max_epochs, num_classes)\n",
    "    trainer.fit(model, train_loader, test_loader)\n",
    "    #model = Trainer_class2.load_from_checkpoint(pretrained_filename, model=model)\n",
    "    print(f'Done with model: {model_name}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae77579-32d6-481d-9ecd-2e12cbddeccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "print('Basline Model')\n",
    "print(f'Start Training with Epochs: {nr_epochs}, nr_classes: {nr_classes}, batch_size:{batch_size}, lr: {lr}, weight_decay:{weight_decay}')\n",
    "\n",
    "\n",
    "for subject_test_int in [1,2,3,15,13]:\n",
    "\n",
    "    name_ds = \"Basline_SEED_p\"+str(subject_test_int)\n",
    "    print(f'current model: {name_ds}')\n",
    "    \n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, name_ds+'-v1.ckpt')\n",
    "    print(pretrained_filename)\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f'Model {name_ds} already exist')\n",
    "        continue\n",
    "    else:\n",
    "        wandb_logger = WandbLogger(log_model=\"all\", project=project_name, name=name_ds)\n",
    "    \n",
    "        subject_train = 'Baseline_AllSubj-'+str(subject_test_int)\n",
    "        subject_test = 'Baseline_Subj'+str(subject_test_int)\n",
    "        \n",
    "        test_dataset = Datasets[subject_test_int-1]\n",
    "        train = [Datasets[i] for i in range(15) if i != subject_test_int-1]\n",
    "        train_dataset = ConcatDataset(train)\n",
    "          \n",
    "        model_baseline = ArjunViT(chunk_size=5,\n",
    "                 t_patch_size=1,\n",
    "                 num_electrodes=channels,\n",
    "                 num_classes=nr_classes)\n",
    "        \n",
    "        #Get only X-% of samples per label:\n",
    "        #sub_train_set = get_smaller_dataset(train_feats_simclr, num_eegs_per_label)\n",
    "\n",
    "\n",
    "        \n",
    "        print(f'Start Training on all, except subj: {subject_test_int}')\n",
    "        print(f'Model_name: {name_ds}')\n",
    "        \n",
    "        train_supervised(model = model_baseline, \n",
    "                        batch_size=batch_size,\n",
    "                        max_epochs=nr_epochs,\n",
    "                        train_data=train_dataset,\n",
    "                        test_data=test_dataset,\n",
    "                        model_name=f'{name_ds}',\n",
    "                        num_classes=nr_classes,\n",
    "                        lr=lr,\n",
    "                        weight_decay=weight_decay,\n",
    "                        logger = wandb_logger,\n",
    "                        )\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cca6634-5958-4ecf-a46a-22934d8eda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from torchmetrics import F1Score\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "def get_results_per_model(model, test_loader):\n",
    "\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    \n",
    "    # iterate over test data\n",
    "    for inputs, labels in test_loader:\n",
    "            inputs = inputs.squeeze()\n",
    "            output = model.model(inputs) # Feed Network\n",
    "            output = output.squeeze()\n",
    "            output = torch.argmax(output, dim=1, keepdim=True).numpy()\n",
    "\n",
    "            y_pred.extend(output) # Save Prediction\n",
    "            \n",
    "            labels = labels.data.cpu().numpy()\n",
    "            y_true.extend(labels) # Save Truth\n",
    "\n",
    "    return y_pred,y_true\n",
    "\n",
    "def get_average_confusion_matrix(y_pred,y_true):\n",
    "    \n",
    "     # constant for classes\n",
    "    classes = ('Negative', 'Neutral', 'Positive')\n",
    "    \n",
    "    # Build confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
    "                         columns = [i for i in classes])\n",
    "    plt.figure(figsize = (12,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.savefig(model_name+'_output.png')\n",
    "\n",
    "    #F1 Score:\n",
    "    f1 = F1Score(task=\"multiclass\", num_classes=3)\n",
    "    print('F1Score:')\n",
    "    y_pred = torch.tensor(y_pred).squeeze()\n",
    "    F1 = f1(y_pred,torch.tensor(y_true))\n",
    "    print(F1)\n",
    "    return F1, df_cm, cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d7081a-0668-44f6-b462-c5c4f8d04504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "#SSL DS:\n",
    "#[1,2,3,15,13]\n",
    "model_1 ='./saved_models/Basline_SEED_p1-v1.ckpt'\n",
    "model_2 ='./saved_models/Basline_SEED_p2-v1.ckpt'\n",
    "model_3 ='./saved_models/Basline_SEED_p3-v1.ckpt'\n",
    "model_15 ='./saved_models/Basline_SEED_p15-v1.ckpt'\n",
    "model_13 ='./saved_models/Basline_SEED_p13-v1.ckpt'\n",
    "\n",
    "model_list = [model_1, model_2, model_3, model_15, model_13]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "                     accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                     devices='auto',\n",
    "                     max_epochs=nr_epochs,\n",
    "                    )\n",
    "\n",
    "model_baseline = ArjunViT(chunk_size=5,\n",
    "                 t_patch_size=1,\n",
    "                 num_electrodes=channels,\n",
    "                 num_classes=nr_classes)\n",
    "\n",
    "ypred_all = []\n",
    "for model_name in model_list:\n",
    "    print(f'model: {model_name[-15:]}')\n",
    "    try: \n",
    "        model = Trainer_class2.load_from_checkpoint(model_name, model=model_baseline)\n",
    "        print(f'loaded model successfully')\n",
    "        #print(model)\n",
    "    except Exception as e:\n",
    "        # If an exception occurs, print an error message\n",
    "        print(f\"Error loading model for {model_name}: {e}\")\n",
    "\n",
    "    \n",
    "    # Extract the number from the file name (get the integer between 'p' and '-v1').\n",
    "    extracted_number = int(re.search(r'p(\\d+)-v1', model_name).group(1))\n",
    "    print(f'extraced number: {extracted_number}')  \n",
    "    # Try to load the file\n",
    "    test_dataset = Datasets[extracted_number-1]\n",
    "    \n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                              drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    \n",
    "    # Test best model on test set\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    print(f'test_result: {test_result}')\n",
    "    y_pred,y_true = get_results_per_model(model, test_loader)\n",
    "    \n",
    "    F1, df_cm, cf_matrix = get_average_confusion_matrix(y_pred,y_true)\n",
    "    print(f'F1: {F1}')\n",
    "    print(f'df_cm: {df_cm}')\n",
    "    ypred_all.append(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "807b83f7-d5d3-471d-827e-3c96f4276359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average over all k-folds:           Negative   Neutral  Positive\n",
      "Negative  0.615952  0.366667  0.017381\n",
      "Neutral   0.339915  0.575725  0.084360\n",
      "Positive  0.313846  0.282792  0.403362\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take the average of the DataFrames\n",
    "average_df = pd.concat(ypred_all).groupby(level=0).mean()\n",
    "print(f'Average over all k-folds: {average_df}')\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(average_df, annot=True)\n",
    "plt.savefig('Average_kfold_baseline.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc94b6-da88-4619-aff2-94dc28f15cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
